{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "from core.sae import SparseAutoEncoder\n",
    "from core.config import SAEConfig, ActivationStoreConfig, LanguageModelConfig\n",
    "from core.activation.token_source import TokenSource\n",
    "from core.activation.activation_source import TokenActivationSource\n",
    "from core.activation.activation_store import ActivationStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2 into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "hf_model = AutoModelForCausalLM.from_pretrained('gpt2')\n",
    "model = HookedTransformer.from_pretrained('gpt2', device=device, hf_model=hf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook_point = 'blocks.1.hook_mlp_out'\n",
    "sentences = [\n",
    "    \"I went to the store [\",\n",
    "    \"She told me a secret [I promised not to tell\",\n",
    "    \"The cat [which was black\",\n",
    "    \"We planned a trip to the beach [but it rained\",\n",
    "    \"He bought a new car [a red convertible\",\n",
    "    \"They decided to cancel the event [due to low attendance\",\n",
    "    \"She brought her dog [a golden retriever\",\n",
    "    \"He received a gift [a watch\",\n",
    "    \"The book [which was on sale\",\n",
    "    \"They attended a party [hosted by their neighbors\"\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_file_names = ['ft-L0A-bs-4096-lr-8e-5-32x', 'ft-L0M-bs-4096-lr-8e-5-32x',\n",
    "                   'ft-L1A-bs-4096-lr-8e-5-32x', 'ft-L1M-bs-4096-lr-8e-5-32x',\n",
    "                   'ft-L2A-bs-4096-lr-8e-5-32x', 'ft-L2M-bs-4096-lr-8e-5-32x',\n",
    "                   'ft-L3A-bs-4096-lr-8e-5-32x', 'ft-L3M-bs-4096-lr-8e-5-32x',\n",
    "                   'ft-L4A-bs-4096-lr-8e-5-32x', 'ft-L4M-bs-4096-lr-8e-5-32x',\n",
    "                   'ft-L5A-bs-4096-lr-8e-5-32x', 'ft-L5M-bs-4096-lr-8e-5-32x',\n",
    "                   'ft-L6A-bs-4096-lr-8e-5-32x', 'ft-L6M-bs-4096-lr-8e-5-32x',\n",
    "                   'ft-L7A-bs-4096-lr-8e-5-32x', 'ft-L7M-bs-4096-lr-8e-5-32x',\n",
    "                   'ft-L8A-bs-4096-lr-8e-5-32x', 'ft-L8M-bs-4096-lr-8e-5-32x',\n",
    "                   'ft-L9A-bs-4096-lr-8e-5-32x', 'ft-L9M-bs-4096-lr-8e-5-32x',\n",
    "                   'ft-L10A-bs-4096-lr-8e-5-32x', 'ft-L10M-bs-4096-lr-8e-5-32x',\n",
    "                   'ft-L11A-bs-4096-lr-8e-5-32x', 'ft-L11M-bs-4096-lr-8e-5-32x',]\n",
    "\n",
    "layer_to_dict = {'blocks.0.hook_attn_out': 'ft-L0A-bs-4096-lr-8e-5-32x', 'blocks.0.hook_mlp_out': 'ft-L0M-bs-4096-lr-8e-5-32x',\n",
    "                 'blocks.1.hook_attn_out': 'ft-L1A-bs-4096-lr-8e-5-32x', 'blocks.1.hook_mlp_out': 'ft-L1M-bs-4096-lr-8e-5-32x',\n",
    "                 'blocks.2.hook_attn_out': 'ft-L2A-bs-4096-lr-8e-5-32x', 'blocks.2.hook_mlp_out': 'ft-L2M-bs-4096-lr-8e-5-32x',\n",
    "                 'blocks.3.hook_attn_out': 'ft-L3A-bs-4096-lr-8e-5-32x', 'blocks.3.hook_mlp_out': 'ft-L3M-bs-4096-lr-8e-5-32x',\n",
    "                 'blocks.4.hook_attn_out': 'ft-L4A-bs-4096-lr-8e-5-32x', 'blocks.4.hook_mlp_out': 'ft-L4M-bs-4096-lr-8e-5-32x',\n",
    "                 'blocks.5.hook_attn_out': 'ft-L5A-bs-4096-lr-8e-5-32x', 'blocks.5.hook_mlp_out': 'ft-L5M-bs-4096-lr-8e-5-32x',\n",
    "                 'blocks.6.hook_attn_out': 'ft-L6A-bs-4096-lr-8e-5-32x', 'blocks.6.hook_mlp_out': 'ft-L6M-bs-4096-lr-8e-5-32x',\n",
    "                 'blocks.7.hook_attn_out': 'ft-L7A-bs-4096-lr-8e-5-32x', 'blocks.7.hook_mlp_out': 'ft-L7M-bs-4096-lr-8e-5-32x',\n",
    "                 'blocks.8.hook_attn_out': 'ft-L8A-bs-4096-lr-8e-5-32x', 'blocks.8.hook_mlp_out': 'ft-L8M-bs-4096-lr-8e-5-32x',\n",
    "                 'blocks.9.hook_attn_out': 'ft-L9A-bs-4096-lr-8e-5-32x', 'blocks.9.hook_mlp_out': 'ft-L9M-bs-4096-lr-8e-5-32x',\n",
    "                 'blocks.10.hook_attn_out': 'ft-L10A-bs-4096-lr-8e-5-32x', 'blocks.10.hook_mlp_out': 'ft-L10M-bs-4096-lr-8e-5-32x',\n",
    "                 'blocks.11.hook_attn_out': 'ft-L11A-bs-4096-lr-8e-5-32x', 'blocks.11.hook_mlp_out': 'ft-L11M-bs-4096-lr-8e-5-32x',}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAEconfig = SAEConfig(**SAEConfig.get_hyperparameters(layer_to_dict[hook_point], '/remote-home/share/research/mechinterp/gpt2-dictionary/ftresults', 'final.pt'), device=device, seed=42,)\n",
    "sae = SparseAutoEncoder(cfg=SAEconfig)\n",
    "sae.load_state_dict(torch.load(sae.cfg.from_pretrained_path, map_location=sae.cfg.device)[\"sae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 12, 24576])\n"
     ]
    }
   ],
   "source": [
    "tokens = model.to_tokens(sentences, prepend_bos=False)\n",
    "_, cache = model.run_with_cache(tokens, names_filter=[hook_point])\n",
    "activation = cache[hook_point]\n",
    "\n",
    "_, (_, aux) = sae(activation)\n",
    "feature_acts = aux[\"feature_acts\"]\n",
    "print(feature_acts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([10, 24576])\n",
      "tensor([21675,  4230, 15926, 21732,     3,     7,     8,     1,     4,     9])\n",
      "tensor([10.4506,  2.8440,  1.0607,  0.3649,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000])\n"
     ]
    }
   ],
   "source": [
    "# 单个符号\n",
    "special_token = model.to_tokens(' [', prepend_bos=False)[0].item()\n",
    "match = torch.eq(tokens, special_token)\n",
    "indices = torch.nonzero(match)  # 只取每行第一个匹配的索引\n",
    "print(indices.shape[0])\n",
    "features = torch.tensor([feature_acts[indices[i][0]][indices[i][1]].tolist() for i in range(0, indices.shape[0])])\n",
    "print(features.shape)\n",
    "min_features, _ = torch.min(features, dim=0)\n",
    "top_values, top_indices = torch.topk(min_features, k = 10)\n",
    "print(top_indices)\n",
    "print(top_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([22223,  3220, 21675,  4230, 15074,   464, 15926,  1231, 17077, 20381],\n",
      "       device='cuda:6')\n",
      "tensor([14.5024, 13.3414, 10.4506,  2.8440,  2.2064,  1.5701,  1.0607,  0.9122,\n",
      "         0.6146,  0.4771], device='cuda:6', grad_fn=<TopkBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 整体语义\n",
    "temp, _ = torch.max(feature_acts, dim=1)\n",
    "final, _ = torch.min(temp, dim=0)\n",
    "top_values, top_indices = torch.topk(final, k = 10)\n",
    "print(top_indices)\n",
    "print(top_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wjx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
