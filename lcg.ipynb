{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2 into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "from typing import Any, Callable\n",
    "\n",
    "sys.path.insert(0, os.getcwd())\n",
    "\n",
    "from HookedTransformer import HookedTransformer\n",
    "# from transformer_lens import HookedTransformer\n",
    "\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "import networkx as nx\n",
    "import random\n",
    "import math\n",
    "import pickle\n",
    "import dataclasses\n",
    "import numpy as np\n",
    "\n",
    "from einops import repeat\n",
    "\n",
    "import plotly.express as px\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from core.config import SAEConfig\n",
    "from core.sae import SparseAutoEncoder\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "hf_model = AutoModelForCausalLM.from_pretrained('gpt2')\n",
    "model = HookedTransformer.from_pretrained('gpt2', device=device, hf_model=hf_model)\n",
    "\n",
    "def check_all_close():\n",
    "\timport transformer_lens\n",
    "\torigin_tl_model = transformer_lens.HookedTransformer.from_pretrained('gpt2', device=device, hf_model=hf_model)\n",
    "\tlogits = model(model.to_tokens('Hello, World.'))\n",
    "\torigin_logits = origin_tl_model(origin_tl_model.to_tokens('Hello, World.'))\n",
    "\tassert torch.allclose(logits, origin_logits, atol=1e-4), f\"Logits are not close: {logits} != {origin_logits}\"\n",
    "\n",
    "# input = model.to_tokens(\" OpenMoss! OpenMoss! OpenMoss!\", prepend_bos=False)\n",
    "# input = model.to_tokens(\"Outside [Inside] Outside\", prepend_bos=False)\n",
    "# input = model.to_tokens(\"0 0 [1 1 1 [2] 3] 4\", prepend_bos=False)\n",
    "# input = model.to_tokens(\"Video in WebM support: Your browser doesn't support HTML5 video in WebM.\", prepend_bos=False)\n",
    "# input = model.to_tokens(\"Form-fitting TrekDry helps keep hands cool and comfortable. Form-fitting TrekDry material is lightweight and breathable.\", prepend_bos=False)\n",
    "# input = model.to_tokens(\" it was its command line interface. You get so much leverage by being able to scaffold a [Inner Inner] A B A\", prepend_bos=False)\n",
    "# input = model.to_tokens(\"[[[ OpenMoss ]]] OpenMoss Open Moss ]\", prepend_bos=False)\n",
    "# input = model.to_tokens(\"Fruits:\\n\\napple red\\n\\nbanana yellow\\n\\ngrape purple\", prepend_bos=False)\n",
    "# input = model.to_tokens(\"Fruits:\\n\\nbanana yellow\\n\\napple red\\n\\ngrape purple\", prepend_bos=False)\n",
    "# input = model.to_tokens(\"Youâ€™re used to endlessly circular debates where Republican shills and Democratic shills\", prepend_bos=False)\n",
    "# input = model.to_tokens(\"Afterwards, Alice and Tom went to the shop. Tom gave a bunch of flowers to\", prepend_bos=False)\n",
    "# input = model.to_tokens(\"Afterwards, Tom and Alice went to the shop. Tom gave a bunch of flowers to\", prepend_bos=False)\n",
    "# input = model.to_tokens(\"When Mary and John went to the store, Mary gave a bottle of milk to\", prepend_bos=False)\n",
    "# input = model.to_tokens(\"When Mary and John went to the store, John gave a bottle of milk to\", prepend_bos=False)\n",
    "# input = model.to_tokens(\"When John and Mary went to the store, John gave a bottle of milk to\", prepend_bos=False)\n",
    "# input = model.to_tokens(\"20 Parts Rosemary, 8 Parts Grapefruit\", prepend_bos=False)\n",
    "\n",
    "# answer = model.to_tokens(\" Mary\", prepend_bos=False)\n",
    "# assert answer.size(0) == 1\n",
    "# logits, cache = model.run_with_cache(input)\n",
    "# logits = logits[0, -1, answer.item()]\n",
    "# print(logits)\n",
    "# logits.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['hook_embed', 'hook_pos_embed', 'blocks.0.hook_resid_pre', 'blocks.0.ln1.hook_scale', 'blocks.0.ln1.hook_normalized', 'blocks.0.attn.hook_q', 'blocks.0.attn.hook_k', 'blocks.0.attn.hook_v', 'blocks.0.attn.hook_attn_scores', 'blocks.0.attn.hook_pattern', 'blocks.0.attn.hook_z', 'blocks.0.hook_attn_out', 'blocks.0.hook_attn_feature_acts', 'blocks.0.hook_attn_sae_error', 'blocks.0.hook_resid_mid', 'blocks.0.ln2.hook_scale', 'blocks.0.ln2.hook_normalized', 'blocks.0.mlp.hook_pre', 'blocks.0.mlp.hook_post', 'blocks.0.hook_mlp_out', 'blocks.0.hook_mlp_feature_acts', 'blocks.0.hook_mlp_sae_error', 'blocks.0.hook_resid_post', 'blocks.1.hook_resid_pre', 'blocks.1.ln1.hook_scale', 'blocks.1.ln1.hook_normalized', 'blocks.1.attn.hook_q', 'blocks.1.attn.hook_k', 'blocks.1.attn.hook_v', 'blocks.1.attn.hook_attn_scores', 'blocks.1.attn.hook_pattern', 'blocks.1.attn.hook_z', 'blocks.1.hook_attn_out', 'blocks.1.hook_attn_feature_acts', 'blocks.1.hook_attn_sae_error', 'blocks.1.hook_resid_mid', 'blocks.1.ln2.hook_scale', 'blocks.1.ln2.hook_normalized', 'blocks.1.mlp.hook_pre', 'blocks.1.mlp.hook_post', 'blocks.1.hook_mlp_out', 'blocks.1.hook_mlp_feature_acts', 'blocks.1.hook_mlp_sae_error', 'blocks.1.hook_resid_post', 'blocks.2.hook_resid_pre', 'blocks.2.ln1.hook_scale', 'blocks.2.ln1.hook_normalized', 'blocks.2.attn.hook_q', 'blocks.2.attn.hook_k', 'blocks.2.attn.hook_v', 'blocks.2.attn.hook_attn_scores', 'blocks.2.attn.hook_pattern', 'blocks.2.attn.hook_z', 'blocks.2.hook_attn_out', 'blocks.2.hook_attn_feature_acts', 'blocks.2.hook_attn_sae_error', 'blocks.2.hook_resid_mid', 'blocks.2.ln2.hook_scale', 'blocks.2.ln2.hook_normalized', 'blocks.2.mlp.hook_pre', 'blocks.2.mlp.hook_post', 'blocks.2.hook_mlp_out', 'blocks.2.hook_mlp_feature_acts', 'blocks.2.hook_mlp_sae_error', 'blocks.2.hook_resid_post', 'blocks.3.hook_resid_pre', 'blocks.3.ln1.hook_scale', 'blocks.3.ln1.hook_normalized', 'blocks.3.attn.hook_q', 'blocks.3.attn.hook_k', 'blocks.3.attn.hook_v', 'blocks.3.attn.hook_attn_scores', 'blocks.3.attn.hook_pattern', 'blocks.3.attn.hook_z', 'blocks.3.hook_attn_out', 'blocks.3.hook_attn_feature_acts', 'blocks.3.hook_attn_sae_error', 'blocks.3.hook_resid_mid', 'blocks.3.ln2.hook_scale', 'blocks.3.ln2.hook_normalized', 'blocks.3.mlp.hook_pre', 'blocks.3.mlp.hook_post', 'blocks.3.hook_mlp_out', 'blocks.3.hook_mlp_feature_acts', 'blocks.3.hook_mlp_sae_error', 'blocks.3.hook_resid_post', 'blocks.4.hook_resid_pre', 'blocks.4.ln1.hook_scale', 'blocks.4.ln1.hook_normalized', 'blocks.4.attn.hook_q', 'blocks.4.attn.hook_k', 'blocks.4.attn.hook_v', 'blocks.4.attn.hook_attn_scores', 'blocks.4.attn.hook_pattern', 'blocks.4.attn.hook_z', 'blocks.4.hook_attn_out', 'blocks.4.hook_attn_feature_acts', 'blocks.4.hook_attn_sae_error', 'blocks.4.hook_resid_mid', 'blocks.4.ln2.hook_scale', 'blocks.4.ln2.hook_normalized', 'blocks.4.mlp.hook_pre', 'blocks.4.mlp.hook_post', 'blocks.4.hook_mlp_out', 'blocks.4.hook_mlp_feature_acts', 'blocks.4.hook_mlp_sae_error', 'blocks.4.hook_resid_post', 'blocks.5.hook_resid_pre', 'blocks.5.ln1.hook_scale', 'blocks.5.ln1.hook_normalized', 'blocks.5.attn.hook_q', 'blocks.5.attn.hook_k', 'blocks.5.attn.hook_v', 'blocks.5.attn.hook_attn_scores', 'blocks.5.attn.hook_pattern', 'blocks.5.attn.hook_z', 'blocks.5.hook_attn_out', 'blocks.5.hook_attn_feature_acts', 'blocks.5.hook_attn_sae_error', 'blocks.5.hook_resid_mid', 'blocks.5.ln2.hook_scale', 'blocks.5.ln2.hook_normalized', 'blocks.5.mlp.hook_pre', 'blocks.5.mlp.hook_post', 'blocks.5.hook_mlp_out', 'blocks.5.hook_mlp_feature_acts', 'blocks.5.hook_mlp_sae_error', 'blocks.5.hook_resid_post', 'blocks.6.hook_resid_pre', 'blocks.6.ln1.hook_scale', 'blocks.6.ln1.hook_normalized', 'blocks.6.attn.hook_q', 'blocks.6.attn.hook_k', 'blocks.6.attn.hook_v', 'blocks.6.attn.hook_attn_scores', 'blocks.6.attn.hook_pattern', 'blocks.6.attn.hook_z', 'blocks.6.hook_attn_out', 'blocks.6.hook_attn_feature_acts', 'blocks.6.hook_attn_sae_error', 'blocks.6.hook_resid_mid', 'blocks.6.ln2.hook_scale', 'blocks.6.ln2.hook_normalized', 'blocks.6.mlp.hook_pre', 'blocks.6.mlp.hook_post', 'blocks.6.hook_mlp_out', 'blocks.6.hook_mlp_feature_acts', 'blocks.6.hook_mlp_sae_error', 'blocks.6.hook_resid_post', 'blocks.7.hook_resid_pre', 'blocks.7.ln1.hook_scale', 'blocks.7.ln1.hook_normalized', 'blocks.7.attn.hook_q', 'blocks.7.attn.hook_k', 'blocks.7.attn.hook_v', 'blocks.7.attn.hook_attn_scores', 'blocks.7.attn.hook_pattern', 'blocks.7.attn.hook_z', 'blocks.7.hook_attn_out', 'blocks.7.hook_attn_feature_acts', 'blocks.7.hook_attn_sae_error', 'blocks.7.hook_resid_mid', 'blocks.7.ln2.hook_scale', 'blocks.7.ln2.hook_normalized', 'blocks.7.mlp.hook_pre', 'blocks.7.mlp.hook_post', 'blocks.7.hook_mlp_out', 'blocks.7.hook_mlp_feature_acts', 'blocks.7.hook_mlp_sae_error', 'blocks.7.hook_resid_post', 'blocks.8.hook_resid_pre', 'blocks.8.ln1.hook_scale', 'blocks.8.ln1.hook_normalized', 'blocks.8.attn.hook_q', 'blocks.8.attn.hook_k', 'blocks.8.attn.hook_v', 'blocks.8.attn.hook_attn_scores', 'blocks.8.attn.hook_pattern', 'blocks.8.attn.hook_z', 'blocks.8.hook_attn_out', 'blocks.8.hook_attn_feature_acts', 'blocks.8.hook_attn_sae_error', 'blocks.8.hook_resid_mid', 'blocks.8.ln2.hook_scale', 'blocks.8.ln2.hook_normalized', 'blocks.8.mlp.hook_pre', 'blocks.8.mlp.hook_post', 'blocks.8.hook_mlp_out', 'blocks.8.hook_mlp_feature_acts', 'blocks.8.hook_mlp_sae_error', 'blocks.8.hook_resid_post', 'blocks.9.hook_resid_pre', 'blocks.9.ln1.hook_scale', 'blocks.9.ln1.hook_normalized', 'blocks.9.attn.hook_q', 'blocks.9.attn.hook_k', 'blocks.9.attn.hook_v', 'blocks.9.attn.hook_attn_scores', 'blocks.9.attn.hook_pattern', 'blocks.9.attn.hook_z', 'blocks.9.hook_attn_out', 'blocks.9.hook_attn_feature_acts', 'blocks.9.hook_attn_sae_error', 'blocks.9.hook_resid_mid', 'blocks.9.ln2.hook_scale', 'blocks.9.ln2.hook_normalized', 'blocks.9.mlp.hook_pre', 'blocks.9.mlp.hook_post', 'blocks.9.hook_mlp_out', 'blocks.9.hook_mlp_feature_acts', 'blocks.9.hook_mlp_sae_error', 'blocks.9.hook_resid_post', 'blocks.10.hook_resid_pre', 'blocks.10.ln1.hook_scale', 'blocks.10.ln1.hook_normalized', 'blocks.10.attn.hook_q', 'blocks.10.attn.hook_k', 'blocks.10.attn.hook_v', 'blocks.10.attn.hook_attn_scores', 'blocks.10.attn.hook_pattern', 'blocks.10.attn.hook_z', 'blocks.10.hook_attn_out', 'blocks.10.hook_attn_feature_acts', 'blocks.10.hook_attn_sae_error', 'blocks.10.hook_resid_mid', 'blocks.10.ln2.hook_scale', 'blocks.10.ln2.hook_normalized', 'blocks.10.mlp.hook_pre', 'blocks.10.mlp.hook_post', 'blocks.10.hook_mlp_out', 'blocks.10.hook_mlp_feature_acts', 'blocks.10.hook_mlp_sae_error', 'blocks.10.hook_resid_post', 'blocks.11.hook_resid_pre', 'blocks.11.ln1.hook_scale', 'blocks.11.ln1.hook_normalized', 'blocks.11.attn.hook_q', 'blocks.11.attn.hook_k', 'blocks.11.attn.hook_v', 'blocks.11.attn.hook_attn_scores', 'blocks.11.attn.hook_pattern', 'blocks.11.attn.hook_z', 'blocks.11.hook_attn_out', 'blocks.11.hook_attn_feature_acts', 'blocks.11.hook_attn_sae_error', 'blocks.11.hook_resid_mid', 'blocks.11.ln2.hook_scale', 'blocks.11.ln2.hook_normalized', 'blocks.11.mlp.hook_pre', 'blocks.11.mlp.hook_post', 'blocks.11.hook_mlp_out', 'blocks.11.hook_mlp_feature_acts', 'blocks.11.hook_mlp_sae_error', 'blocks.11.hook_resid_post', 'ln_final.hook_scale', 'ln_final.hook_normalized', 'ln_final.hook_normalized_grad', 'blocks.11.hook_resid_post_grad', 'blocks.11.hook_mlp_sae_error_grad', 'blocks.11.hook_mlp_feature_acts_grad', 'blocks.11.hook_resid_mid_grad', 'blocks.11.hook_attn_sae_error_grad', 'blocks.11.hook_attn_feature_acts_grad', 'blocks.11.hook_attn_out_grad', 'blocks.11.attn.hook_z_grad', 'blocks.11.attn.hook_v_grad', 'blocks.11.ln1.hook_normalized_grad', 'blocks.11.hook_resid_pre_grad', 'blocks.10.hook_resid_post_grad', 'blocks.10.hook_mlp_sae_error_grad', 'blocks.10.hook_mlp_feature_acts_grad', 'blocks.10.hook_resid_mid_grad', 'blocks.10.hook_attn_sae_error_grad', 'blocks.10.hook_attn_feature_acts_grad', 'blocks.10.hook_attn_out_grad', 'blocks.10.attn.hook_z_grad', 'blocks.10.attn.hook_v_grad', 'blocks.10.ln1.hook_normalized_grad', 'blocks.10.hook_resid_pre_grad', 'blocks.9.hook_resid_post_grad', 'blocks.9.hook_mlp_sae_error_grad', 'blocks.9.hook_mlp_feature_acts_grad', 'blocks.9.hook_resid_mid_grad', 'blocks.9.hook_attn_sae_error_grad', 'blocks.9.hook_attn_feature_acts_grad', 'blocks.9.hook_attn_out_grad', 'blocks.9.attn.hook_z_grad', 'blocks.9.attn.hook_v_grad', 'blocks.9.ln1.hook_normalized_grad', 'blocks.9.hook_resid_pre_grad', 'blocks.8.hook_resid_post_grad', 'blocks.8.hook_mlp_sae_error_grad', 'blocks.8.hook_mlp_feature_acts_grad', 'blocks.8.hook_resid_mid_grad', 'blocks.8.hook_attn_sae_error_grad', 'blocks.8.hook_attn_feature_acts_grad', 'blocks.8.hook_attn_out_grad', 'blocks.8.attn.hook_z_grad', 'blocks.8.attn.hook_v_grad', 'blocks.8.ln1.hook_normalized_grad', 'blocks.8.hook_resid_pre_grad', 'blocks.7.hook_resid_post_grad', 'blocks.7.hook_mlp_sae_error_grad', 'blocks.7.hook_mlp_feature_acts_grad', 'blocks.7.hook_resid_mid_grad', 'blocks.7.hook_attn_sae_error_grad', 'blocks.7.hook_attn_feature_acts_grad', 'blocks.7.hook_attn_out_grad', 'blocks.7.attn.hook_z_grad', 'blocks.7.attn.hook_v_grad', 'blocks.7.ln1.hook_normalized_grad', 'blocks.7.hook_resid_pre_grad', 'blocks.6.hook_resid_post_grad', 'blocks.6.hook_mlp_sae_error_grad', 'blocks.6.hook_mlp_feature_acts_grad', 'blocks.6.hook_resid_mid_grad', 'blocks.6.hook_attn_sae_error_grad', 'blocks.6.hook_attn_feature_acts_grad', 'blocks.6.hook_attn_out_grad', 'blocks.6.attn.hook_z_grad', 'blocks.6.attn.hook_v_grad', 'blocks.6.ln1.hook_normalized_grad', 'blocks.6.hook_resid_pre_grad', 'blocks.5.hook_resid_post_grad', 'blocks.5.hook_mlp_sae_error_grad', 'blocks.5.hook_mlp_feature_acts_grad', 'blocks.5.hook_resid_mid_grad', 'blocks.5.hook_attn_sae_error_grad', 'blocks.5.hook_attn_feature_acts_grad', 'blocks.5.hook_attn_out_grad', 'blocks.5.attn.hook_z_grad', 'blocks.5.attn.hook_v_grad', 'blocks.5.ln1.hook_normalized_grad', 'blocks.5.hook_resid_pre_grad', 'blocks.4.hook_resid_post_grad', 'blocks.4.hook_mlp_sae_error_grad', 'blocks.4.hook_mlp_feature_acts_grad', 'blocks.4.hook_resid_mid_grad', 'blocks.4.hook_attn_sae_error_grad', 'blocks.4.hook_attn_feature_acts_grad', 'blocks.4.hook_attn_out_grad', 'blocks.4.attn.hook_z_grad', 'blocks.4.attn.hook_v_grad', 'blocks.4.ln1.hook_normalized_grad', 'blocks.4.hook_resid_pre_grad', 'blocks.3.hook_resid_post_grad', 'blocks.3.hook_mlp_sae_error_grad', 'blocks.3.hook_mlp_feature_acts_grad', 'blocks.3.hook_resid_mid_grad', 'blocks.3.hook_attn_sae_error_grad', 'blocks.3.hook_attn_feature_acts_grad', 'blocks.3.hook_attn_out_grad', 'blocks.3.attn.hook_z_grad', 'blocks.3.attn.hook_v_grad', 'blocks.3.ln1.hook_normalized_grad', 'blocks.3.hook_resid_pre_grad', 'blocks.2.hook_resid_post_grad', 'blocks.2.hook_mlp_sae_error_grad', 'blocks.2.hook_mlp_feature_acts_grad', 'blocks.2.hook_resid_mid_grad', 'blocks.2.hook_attn_sae_error_grad', 'blocks.2.hook_attn_feature_acts_grad', 'blocks.2.hook_attn_out_grad', 'blocks.2.attn.hook_z_grad', 'blocks.2.attn.hook_v_grad', 'blocks.2.ln1.hook_normalized_grad', 'blocks.2.hook_resid_pre_grad', 'blocks.1.hook_resid_post_grad', 'blocks.1.hook_mlp_sae_error_grad', 'blocks.1.hook_mlp_feature_acts_grad', 'blocks.1.hook_resid_mid_grad', 'blocks.1.hook_attn_sae_error_grad', 'blocks.1.hook_attn_feature_acts_grad', 'blocks.1.hook_attn_out_grad', 'blocks.1.attn.hook_z_grad', 'blocks.1.attn.hook_v_grad', 'blocks.1.ln1.hook_normalized_grad', 'blocks.1.hook_resid_pre_grad', 'blocks.0.hook_resid_post_grad', 'blocks.0.hook_mlp_sae_error_grad', 'blocks.0.hook_mlp_feature_acts_grad', 'blocks.0.hook_resid_mid_grad', 'blocks.0.hook_attn_sae_error_grad', 'blocks.0.hook_attn_feature_acts_grad', 'blocks.0.hook_attn_out_grad', 'blocks.0.attn.hook_z_grad', 'blocks.0.attn.hook_v_grad', 'blocks.0.ln1.hook_normalized_grad', 'blocks.0.hook_resid_pre_grad', 'hook_pos_embed_grad', 'hook_embed_grad'])\n"
     ]
    }
   ],
   "source": [
    "model.cfg.detach_pattern = True\n",
    "model.cfg.add_sae_error = True\n",
    "model.cfg.prune_on_backward = True\n",
    "model.cfg.prune_on_backward_threshold = 0\n",
    "\n",
    "input = model.to_tokens(\"When Mary and John went to the store, John gave a bottle of milk to\", prepend_bos=False)\n",
    "# input = model.to_tokens(\"When John and Mary went to the store, John gave a bottle of milk to\", prepend_bos=False)\n",
    "# input = model.to_tokens(\"20 Parts Rosemary, 8 Parts Grapefruit\", prepend_bos=False)\n",
    "\n",
    "answer = model.to_tokens(\" Mary\", prepend_bos=False)\n",
    "wrong_answer = model.to_tokens(\" John\", prepend_bos=False)\n",
    "assert answer.size(0) == 1\n",
    "cache = model.add_caching_hooks(incl_bwd=True)\n",
    "logits = model(input)\n",
    "true_logits = logits[0, -1, answer.item()]\n",
    "wrong_logits = logits[0, -1, wrong_answer.item()]\n",
    "# print(true_logits, wrong_logits)\n",
    "model.zero_grad()\n",
    "# (true_logits - wrong_logits).backward()\n",
    "true_logits.backward()\n",
    "# print(true_logits - wrong_logits)\n",
    "print(cache.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([76])\n",
      "tensor([[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000, -29.7685]], device='cuda:0')\n",
      "tensor([ 1.4549e+01, -2.9270e+00, -1.9406e+01,  6.9698e+00,  1.7703e+01,\n",
      "        -6.1183e+00, -1.7864e+01, -3.8680e+00,  3.0710e+01,  1.4790e+01,\n",
      "         9.4041e+00,  9.9815e-01,  6.7224e+00,  4.5646e+00,  8.4201e+00,\n",
      "         1.1727e+01,  1.7822e+01, -1.4535e+01,  1.3341e+01,  4.9351e+01,\n",
      "         4.7184e-01, -1.0326e+01, -2.7224e+01,  9.3938e+00, -1.0659e+01,\n",
      "         3.0703e+01,  3.1021e+01, -1.3589e+01, -1.5131e+01,  6.1209e+00,\n",
      "         1.1390e+01, -7.3911e+00,  2.9981e+01, -9.4408e-01, -5.7104e+00,\n",
      "         1.0971e+01, -7.3077e+02,  1.2999e+01, -5.8169e+00,  1.0843e+01,\n",
      "        -2.1898e+00,  1.5227e+01,  2.1010e+01, -5.0318e+00, -1.5301e+01,\n",
      "        -1.3711e+01, -1.7306e+01, -5.2128e+00,  1.0260e+01, -8.2211e+01,\n",
      "         1.6497e+01,  1.4167e+01,  5.6458e+00, -9.5709e+00,  2.2646e+01,\n",
      "        -2.7454e+00, -7.7990e+00,  1.8354e-01, -6.7601e+00,  1.7704e+01,\n",
      "        -1.0397e+01,  5.9826e+00, -1.1390e+01,  3.5118e+01,  1.5360e+02,\n",
      "        -8.5937e-01,  2.0686e+01,  3.1983e-01,  5.4062e+00, -2.6959e+00,\n",
      "         5.0212e+00, -6.8308e+00, -3.0023e+00, -2.1163e+01, -1.7145e+01,\n",
      "        -2.8642e+01, -1.4401e+01,  2.6232e+01, -8.7926e+00, -1.6883e+01,\n",
      "        -3.3439e+00,  1.6032e+01,  5.1363e+00, -9.6608e+00,  6.9657e+00,\n",
      "         6.6675e+00, -2.7237e+01,  1.6937e+02, -1.5322e+01, -1.8846e+00,\n",
      "        -6.4753e+00, -1.7684e+01,  1.0159e+01, -1.6142e+01, -8.7125e+00,\n",
      "         5.4678e+00, -4.3801e+00,  5.4797e+00, -1.1163e+01, -9.4375e+00,\n",
      "         1.2049e+01,  5.3168e+00,  1.5595e+01,  1.6015e+01,  1.5825e+01,\n",
      "        -5.7225e+00,  1.5000e+01, -8.4062e+00,  1.0390e+01, -1.5726e+01,\n",
      "        -5.1471e+00, -6.3664e+00,  8.2025e+00, -7.0499e-01,  6.1994e+00,\n",
      "         8.2232e+00,  1.1736e+01,  6.2993e+00,  2.0249e+01,  1.1868e+01,\n",
      "         4.6068e+00, -2.6623e+01,  5.5614e-01, -2.4304e+01,  1.1035e+01,\n",
      "        -5.2596e+00,  4.5862e-01,  1.2859e+01,  6.5949e-01, -4.5339e+00,\n",
      "        -1.2467e+01, -2.1501e+01, -2.9831e+01,  2.2809e+01,  1.9730e+00,\n",
      "        -2.1906e+01,  2.7811e+01,  7.2481e+00,  2.6178e+02, -5.8767e+01,\n",
      "         1.5670e+01,  2.8045e+01, -2.7303e+01, -2.8407e+01, -5.0728e+00,\n",
      "        -7.6612e+00, -8.9778e-01,  1.1448e+01,  4.7488e+00,  4.6992e-01,\n",
      "        -6.3037e+00,  1.9054e+01,  3.8435e+01,  1.5134e+01,  3.5794e+01,\n",
      "         3.8353e+00,  3.5844e+01, -9.1522e+00, -2.6732e-01, -5.4672e+00,\n",
      "         3.6979e+01,  2.3060e+00, -8.9225e-01, -1.9482e+01,  4.5602e+00,\n",
      "        -1.0923e+01,  9.6306e+00,  7.0876e+01,  1.0948e+01, -3.2865e+00,\n",
      "         2.6252e+00, -1.5063e+01, -8.8894e+00,  1.1521e+01,  8.2281e+00,\n",
      "        -1.4556e+01,  1.0973e+02, -8.8023e+00,  1.3307e+01, -1.4319e+01,\n",
      "         7.5877e+00, -1.0314e+00,  5.2879e+00, -1.6456e+01, -2.3793e+01,\n",
      "         2.8821e+00,  2.2496e+01, -1.9688e+00, -1.3478e+01, -1.7192e+00,\n",
      "         8.3614e+00,  5.5212e+01,  6.5566e-01,  6.2436e+00, -3.0573e+01,\n",
      "        -1.0014e+01,  4.9459e-01, -6.8920e+00,  6.6098e+00, -1.6512e+01,\n",
      "        -6.6312e+00, -9.5887e+00,  1.1893e+01,  6.5802e+00, -2.1208e+01,\n",
      "         2.4037e+01, -2.0682e+01, -2.5182e+01, -8.8417e+00,  7.2954e-01,\n",
      "        -9.3669e+00,  1.3896e+00,  8.9741e+00,  4.1267e+00, -9.1702e+00,\n",
      "        -7.0350e+00,  1.2037e+01,  1.8372e+01, -3.1160e+01,  1.0235e+01,\n",
      "         8.4735e+00, -1.7330e+01, -3.3896e+01,  1.5526e+01,  5.2776e+00,\n",
      "        -1.9386e+01, -1.8783e+01,  1.2097e+01, -1.8656e+01, -1.1871e+01,\n",
      "        -6.8888e+00,  2.7409e+00, -1.8899e+01, -4.3079e+00,  9.2208e+00,\n",
      "         2.4552e+01,  8.2172e-02, -1.8273e+00,  2.0584e+01, -5.7790e+00,\n",
      "         2.9704e+01, -1.7191e+00,  6.6643e+00,  1.0727e+01, -3.8395e-01,\n",
      "        -3.7792e+00, -1.3632e+00, -6.9839e+00,  7.2479e+00, -1.1605e+01,\n",
      "        -1.4416e+01, -4.2576e+00,  8.8078e+00, -8.0245e+00, -3.7159e+00,\n",
      "         3.5042e+01, -1.3971e+01, -1.2458e+01, -8.6860e+00,  4.6258e+00,\n",
      "        -6.0527e+00,  9.8599e+00, -6.3221e+00,  3.7400e+00, -1.2456e+00,\n",
      "         9.8122e+00,  1.6393e+02,  2.1912e+01, -8.8883e+00,  1.5141e+00,\n",
      "         1.3449e+01,  2.1173e+01, -2.5734e+01,  1.7046e+01,  1.8166e+01,\n",
      "        -1.2041e+01, -3.4036e+00, -1.8996e+01, -1.2962e+01,  1.5541e+01,\n",
      "         1.5155e+01, -8.4829e+00, -1.3346e+01,  9.4013e+00,  3.7763e+01,\n",
      "         6.6029e-01, -8.0386e+00, -1.2762e+01,  2.2018e+01, -1.6662e+01,\n",
      "        -5.4442e+00,  3.2841e+00, -6.8437e+00, -1.2462e+01, -8.5715e+00,\n",
      "         1.0994e+01,  4.0125e+00, -4.0293e+00, -2.4410e-01,  2.0177e+01,\n",
      "         1.9387e+01, -5.2963e+00,  1.4089e+00, -1.4913e+01,  2.3779e+01,\n",
      "        -5.4834e-01, -1.5378e+00, -1.1292e+01, -1.5004e+00,  1.4184e+01,\n",
      "         1.9578e+01,  1.9029e+00, -1.9135e+01,  2.0089e+01,  9.6585e+02,\n",
      "         7.3912e+00, -9.8846e+00,  4.9792e+00, -2.5256e+01, -2.2964e+01,\n",
      "         6.3527e+01, -8.7118e+00, -2.1433e+01, -2.2409e+01, -1.5364e+01,\n",
      "         1.3090e+01, -3.4643e+01, -1.0705e+01,  1.4519e+00, -8.5951e+00,\n",
      "        -1.3072e+01,  2.3274e+01, -1.4002e+01,  1.2587e-01,  8.9643e+00,\n",
      "         6.2546e+00, -3.4012e-01, -1.6126e+01, -6.8330e+00,  3.2761e+00,\n",
      "         3.9460e+00,  3.4970e+00, -1.4363e+01,  4.6279e+00, -1.8029e+01,\n",
      "         2.6807e+01,  1.1234e+01, -4.3996e+01,  1.2435e+01,  3.8340e+00,\n",
      "         9.5334e+00, -2.1281e+01, -1.1554e+01,  1.0614e+00, -1.3779e+01,\n",
      "        -9.2365e+00, -5.8353e+01,  6.2660e+00,  2.7298e+00,  4.1588e+00,\n",
      "         1.6200e+01, -4.6673e+00, -3.2679e+02, -1.2306e+01,  9.8022e+00,\n",
      "         2.7204e+01,  1.3121e+01,  4.1356e+01, -8.1552e+00, -1.9357e-01,\n",
      "        -6.5869e+00,  8.0645e+00,  3.5699e+00,  2.5120e+01,  2.0286e+02,\n",
      "        -2.2882e+01,  1.1694e-01, -2.4548e+01,  9.3010e+00, -1.5819e+01,\n",
      "         3.1388e+00,  1.3098e+01,  5.1135e-01,  1.2082e+01,  2.3431e+00,\n",
      "         2.3762e+01, -3.6040e+00, -8.5320e+00, -3.5808e+00, -2.4452e+01,\n",
      "         9.8299e+00, -7.0194e+00,  4.7292e+00,  7.3320e+01, -2.4868e+00,\n",
      "         8.6657e+00, -4.6973e+00,  9.6879e-01, -1.7822e+01,  7.9339e+00,\n",
      "         9.6002e-02,  9.1489e+00, -2.7068e+00, -2.7795e+01,  1.4553e+01,\n",
      "        -3.8356e+00,  2.2273e+01, -1.1572e+01, -1.1704e+02, -4.3299e+00,\n",
      "        -5.3056e-01,  9.1362e+00,  1.2197e+01,  2.2993e+01, -1.4911e+01,\n",
      "        -1.4504e+01, -1.4245e+00, -2.7459e+01, -3.4083e+00, -1.8737e+01,\n",
      "         1.7467e+01, -1.0750e+00,  5.0310e-01,  1.8911e+01, -8.0280e+00,\n",
      "        -8.4927e+00,  1.0977e+01, -1.8109e+01, -3.0371e+00,  1.1803e+00,\n",
      "        -9.7715e+02, -1.0730e+01, -2.3306e+01, -6.5883e+00,  1.5563e+00,\n",
      "         3.8133e+00,  6.3698e+00,  1.4064e+01,  8.2634e+00, -9.4094e+00,\n",
      "        -1.5469e+01,  1.1535e+01,  2.3060e+01,  1.7574e+01,  7.4365e+00,\n",
      "         6.6682e+00, -1.2791e+01,  4.0272e+01, -1.3996e+01,  1.3288e+00,\n",
      "         1.2995e+01,  2.2663e+01,  1.2595e+01,  6.1205e+00, -1.5023e+01,\n",
      "         1.9293e+01, -8.8445e+00,  4.9604e+01, -2.6355e+01, -3.9686e+00,\n",
      "         3.7046e+00,  1.9658e+01,  2.9579e+00, -1.4757e+01,  3.8917e+00,\n",
      "        -1.1628e+01, -6.5304e+00,  1.4442e+01,  6.3435e+00, -8.8942e+00,\n",
      "        -2.4290e+01,  1.1878e+01, -3.5261e+00, -1.2438e+01, -7.7368e-02,\n",
      "         2.0221e+00,  2.0130e+01, -5.1792e+00, -8.3420e+00,  3.3275e+01,\n",
      "         1.8658e+02,  3.0562e+01,  1.9047e+01, -2.3279e+01,  4.9721e+00,\n",
      "        -1.2523e+01,  9.4329e-01,  4.1015e+00,  1.0867e+00, -2.1424e+01,\n",
      "         4.0313e+00, -8.4307e+00, -7.6497e+00,  1.0350e+01,  8.5756e+00,\n",
      "        -1.8415e+01, -5.8885e+02, -6.0207e-01,  1.8099e+01, -8.3682e+00,\n",
      "        -2.3680e+01,  2.1609e+00,  1.2107e+01,  2.3396e+01,  2.0461e+01,\n",
      "        -2.0220e+01, -3.0214e+00, -8.7267e+00,  6.0028e+00,  1.3299e+01,\n",
      "        -2.4141e+01, -1.5624e+00, -1.6130e+01,  1.4895e+01, -2.6692e+01,\n",
      "        -2.3909e+00, -9.8955e+00,  3.5871e+00,  8.0641e-01,  3.2112e+00,\n",
      "        -1.2826e+01,  1.8085e+00, -1.4605e+01, -7.4283e+00,  1.7594e+00,\n",
      "         5.2641e+01,  6.0975e+00,  1.1533e+02, -7.5819e+00, -7.3891e+00,\n",
      "        -1.8308e+01,  1.2932e+01,  1.5175e+01,  8.0813e-01,  1.7915e+01,\n",
      "        -1.9269e+01,  4.5727e+00, -6.5315e+00,  3.8408e+00,  1.4138e+01,\n",
      "         8.5627e+00, -7.8639e+00,  5.1490e+00, -2.5083e+01, -3.2489e+01,\n",
      "        -2.6512e+00, -2.8989e+00, -1.1583e+01, -1.3318e+00,  9.6385e+00,\n",
      "         1.5697e+01, -4.8622e+01,  6.1158e+00, -1.3284e+01, -4.5998e+01,\n",
      "         1.2428e+00,  9.4002e+00,  7.6685e+00,  3.3017e+00, -1.6313e+01,\n",
      "         5.5601e+00,  3.6519e+00,  1.9522e+01,  9.3534e+00, -1.9144e+01,\n",
      "         3.4776e+00,  6.6629e+00, -1.5183e+01,  2.7776e+00, -2.6412e+00,\n",
      "        -2.7361e+01,  7.6431e+00, -1.7839e+01,  1.3415e+01, -1.9340e+01,\n",
      "        -1.3691e+01, -8.3638e+00, -3.3024e+00, -1.6308e+01,  2.6823e+01,\n",
      "        -8.2323e+00,  5.3098e+00,  1.4434e+01,  2.9873e-02, -4.0965e+00,\n",
      "         2.5458e+01, -7.5814e+00,  1.3091e+01,  1.2834e+01,  3.5695e+00,\n",
      "         7.4673e+00, -2.1196e+01,  1.4903e+01,  6.1439e+00, -5.3922e+00,\n",
      "         1.2347e+01, -6.8871e+00,  1.0233e+01,  4.9386e+00, -1.5484e+01,\n",
      "         1.0880e+00,  1.1794e+01, -4.9872e+00, -8.8848e+00,  9.2994e-01,\n",
      "         3.1230e+01,  4.0393e+00,  5.1477e+00, -2.8160e+02,  8.6026e-01,\n",
      "        -1.0463e+01, -1.4560e+01, -8.9290e+00,  3.2175e+00, -3.2657e+01,\n",
      "         2.4072e+00, -1.7838e+01,  2.3395e+00, -6.8684e+00,  1.5272e+01,\n",
      "        -1.6858e+01, -9.7188e+00, -6.1672e+00, -7.1230e+00,  2.5101e+01,\n",
      "         2.6873e+00, -4.1599e+00,  5.6333e+01,  1.5479e+01, -2.1834e+01,\n",
      "         9.8953e+00, -1.1558e+00,  2.6897e+00,  9.1105e-01,  3.1165e+00,\n",
      "        -6.3353e+00,  6.1934e+00,  1.3740e+01,  1.3082e+01,  1.0682e+01,\n",
      "         4.4557e+01,  5.9784e+00,  9.0656e+00, -1.6635e-01, -1.0688e+01,\n",
      "        -4.5939e+00, -3.3384e+01, -4.5318e+00, -1.4847e+01,  2.8611e+01,\n",
      "        -1.6753e+01, -2.0532e+00, -1.3223e+01,  1.3583e+01, -7.0684e+00,\n",
      "        -1.0345e+00,  1.5827e+01,  8.6454e+00, -3.9945e+00,  8.9120e-01,\n",
      "        -5.3781e+00,  2.3852e+01, -5.5539e+00, -5.6682e-01, -5.1954e+00,\n",
      "        -2.1644e+01,  2.3544e+01,  1.6480e+01,  3.0603e+01, -5.7997e+01,\n",
      "         2.5415e+01, -1.9500e+01, -1.8989e+01,  2.1296e+00,  4.4688e+01,\n",
      "        -6.0349e+00,  2.0980e+01, -1.4226e+01,  4.1834e+00,  1.3255e+01,\n",
      "        -2.9587e+01,  3.6768e+01,  1.9196e+01,  1.4682e+01, -4.8039e+00,\n",
      "         2.9851e+00,  4.8457e+00,  1.0346e+01,  2.0753e+01,  1.7368e+01,\n",
      "        -1.4958e+01, -1.5721e+01, -1.2778e+01,  1.4906e+01,  7.2122e+00,\n",
      "         6.6107e+00,  2.5272e+01,  1.5958e+01,  1.1428e+01, -1.3373e+01,\n",
      "         1.1946e+01, -1.6903e+01, -3.2581e+00, -1.5270e+00, -4.2303e+00,\n",
      "         5.1420e+00,  2.0791e+01, -1.7911e+01,  3.5764e+00,  2.0718e+02,\n",
      "        -1.3722e+00,  6.8948e+00, -7.2821e+00, -1.3479e+01, -5.4796e+00,\n",
      "         1.9790e+01,  1.5124e+01,  1.2588e+01, -3.5667e+01,  8.2715e-01,\n",
      "        -2.3333e+01,  1.9443e+01,  2.6486e+01,  8.0487e+00,  2.8833e+01,\n",
      "         1.8388e+01, -1.5580e+00,  4.0363e-01,  5.3511e+00,  1.0439e+01,\n",
      "         2.5106e+00, -1.2971e+01, -2.8228e+01,  6.8351e+00, -6.8389e+00,\n",
      "         1.6051e+01,  1.8057e+01,  1.4860e+01, -4.3907e+00,  1.7302e+01,\n",
      "         5.8014e+00,  2.6799e+00,  1.4588e+00,  5.6044e+00,  1.0780e+01,\n",
      "         2.4705e+01, -2.5939e+00, -8.4331e-01, -2.7020e+00,  2.4710e+00,\n",
      "        -8.3358e+00, -9.8134e-01,  1.7422e+01,  3.5699e-01, -4.8183e+00,\n",
      "         3.1116e+00, -3.3767e+00, -2.8570e+01, -6.4866e+00,  1.4511e+01,\n",
      "        -2.0141e+01,  1.4048e+01, -5.1896e+01,  1.7468e+01,  1.7843e+00,\n",
      "        -5.1611e+00, -1.2284e+01, -9.8384e+00], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import einops\n",
    "\n",
    "\n",
    "print((cache[\"blocks.5.hook_attn_feature_acts_grad\"] * cache[\"blocks.5.hook_attn_feature_acts\"] > 0)[0][-1].nonzero(as_tuple=True)[0].shape)\n",
    "print(einops.einsum(cache[f\"blocks.11.hook_mlp_sae_error\"], cache[f\"blocks.11.hook_mlp_sae_error_grad\"], \"b l d, b l d -> b l\"))\n",
    "print(cache[f\"blocks.11.hook_mlp_sae_error\"][0][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9762\n",
      "tensor(23.9536, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import einops\n",
    "\n",
    "nodes = 0\n",
    "contribution = torch.zeros_like(true_logits)\n",
    "error_contribution = torch.zeros_like(true_logits)\n",
    "\n",
    "embed_contribution = einops.einsum(cache[\"hook_embed\"], cache[\"hook_embed_grad\"], \"b l d, b l d -> b l\")\n",
    "nodes += (embed_contribution[0] > 0).nonzero(as_tuple=True)[0].shape[0]\n",
    "contribution += embed_contribution[embed_contribution > 0].sum()\n",
    "\n",
    "pos_embed_contribution = einops.einsum(cache[\"hook_pos_embed\"], cache[\"hook_pos_embed_grad\"], \"b l d, b l d -> b l\")\n",
    "nodes += (pos_embed_contribution[0] > 0).nonzero(as_tuple=True)[0].shape[0]\n",
    "contribution += pos_embed_contribution[pos_embed_contribution > 0].sum()\n",
    "\n",
    "for i in range(12):\n",
    "    attn_contribution = einops.einsum(cache[f\"blocks.{i}.hook_attn_feature_acts\"], cache[f\"blocks.{i}.hook_attn_feature_acts_grad\"], \"b l f, b l f -> b l f\")\n",
    "    nodes += (attn_contribution[0] > 0).reshape(-1).nonzero(as_tuple=True)[0].shape[0]\n",
    "\n",
    "    attn_sae_error_contribution = einops.einsum(cache[f\"blocks.{i}.hook_attn_sae_error\"], cache[f\"blocks.{i}.hook_attn_sae_error_grad\"], \"b l d, b l d -> b l\")\n",
    "    error_contribution += attn_sae_error_contribution.sum()\n",
    "\n",
    "    b_V_contribution = einops.einsum(model.blocks[i].attn.b_V, model.blocks[i].attn.b_V.grad, \"h d, h d -> h\")\n",
    "    nodes += (b_V_contribution > 0).nonzero(as_tuple=True)[0].shape[0]\n",
    "    contribution += b_V_contribution[b_V_contribution > 0].sum()\n",
    "\n",
    "    attn_b_E_contribution = einops.einsum(model.blocks[i].attn_sae.encoder_bias, model.blocks[i].attn_sae.encoder_bias.grad, \"f, f -> \")\n",
    "    nodes += (attn_b_E_contribution > 0).nonzero(as_tuple=True)[0].shape[0]\n",
    "    contribution += attn_b_E_contribution[attn_b_E_contribution > 0].sum()\n",
    "\n",
    "    mlp_contribution = einops.einsum(cache[f\"blocks.{i}.hook_mlp_feature_acts\"], cache[f\"blocks.{i}.hook_mlp_feature_acts_grad\"], \"b l f, b l f -> b l\")\n",
    "    nodes += (mlp_contribution[0] > 0).reshape(-1).nonzero(as_tuple=True)[0].shape[0]\n",
    "\n",
    "    mlp_sae_error_contribution = einops.einsum(cache[f\"blocks.{i}.hook_mlp_sae_error\"], cache[f\"blocks.{i}.hook_mlp_sae_error_grad\"], \"b l d, b l d -> b l\")\n",
    "    error_contribution += mlp_sae_error_contribution.sum()\n",
    "\n",
    "    mlp_b_E_contribution = einops.einsum(model.blocks[i].mlp_sae.encoder_bias, model.blocks[i].mlp_sae.encoder_bias.grad, \"f, f -> \")\n",
    "    nodes += (mlp_b_E_contribution > 0).nonzero(as_tuple=True)[0].shape[0]\n",
    "    contribution += mlp_b_E_contribution[mlp_b_E_contribution > 0].sum()\n",
    "\n",
    "print(nodes)\n",
    "print(contribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2317419061.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[102], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    feature_acts = model.L0RPr_sae(cache['blocks.0.hook_resid_pre'])[][1]['feature_acts']\u001b[0m\n\u001b[0m                                                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "feature_acts = model.L0RPr_sae(cache['blocks.0.hook_resid_pre'])[][1]['feature_acts']\n",
    "attr = model.L0RPr_grad[0, :, None, :] * model.L0RPr_sae.decoder[None, :, :] * feature_acts[0, :, :, None]\n",
    "attr = attr.sum(-1).flatten()\n",
    "attr = attr.topk(30)\n",
    "for idx, value in zip(attr.indices, attr.values):\n",
    "\tif value > threshold:\n",
    "\t\ttoken_idx, head_idx = divmod(idx.item(), block.attn_sae.cfg.d_sae)\n",
    "\t\tprint(token_idx, head_idx, value)\n",
    "# (model.L0RPr_grad * model.L0RPr_sae(cache['blocks.0.hook_resid_pre'])[1][1]['x_hat']).sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 'A', 0, 20459): tensor(2.5407, device='cuda:0'),\n",
       " (0, 'A', 3, 11601): tensor(1.8369, device='cuda:0'),\n",
       " (0, 'A', 3, 8101): tensor(0.4803, device='cuda:0'),\n",
       " (0, 'A', 1, 14671): tensor(0.4639, device='cuda:0'),\n",
       " (0, 'A', 8, 23630): tensor(0.4037, device='cuda:0'),\n",
       " (0, 'A', 3, 19931): tensor(0.3042, device='cuda:0'),\n",
       " (0, 'A', 1, 18977): tensor(0.2797, device='cuda:0'),\n",
       " (0, 'A', 8, 19931): tensor(0.2788, device='cuda:0'),\n",
       " (0, 'A', 8, 13741): tensor(0.2101, device='cuda:0'),\n",
       " (0, 'A', 8, 14768): tensor(0.1655, device='cuda:0'),\n",
       " (0, 'A', 1, 4342): tensor(0.1181, device='cuda:0'),\n",
       " (0, 'A', 1, 9204): tensor(0.1048, device='cuda:0'),\n",
       " (0, 'A', 3, 12731): tensor(0.1001, device='cuda:0'),\n",
       " (0, 'A', 1, 6617): tensor(0.0777, device='cuda:0'),\n",
       " (0, 'A', 3, 10869): tensor(0.0759, device='cuda:0'),\n",
       " (0, 'A', 8, 8101): tensor(0.0746, device='cuda:0'),\n",
       " (0, 'A', 0, 14131): tensor(0.0733, device='cuda:0'),\n",
       " (0, 'A', 3, 24377): tensor(0.0725, device='cuda:0'),\n",
       " (0, 'A', 1, 11425): tensor(0.0713, device='cuda:0'),\n",
       " (0, 'A', 8, 11601): tensor(0.0686, device='cuda:0'),\n",
       " (0, 'A', 1, 10869): tensor(0.0670, device='cuda:0'),\n",
       " (0, 'A', 3, 20129): tensor(0.0574, device='cuda:0'),\n",
       " (0, 'A', 1, 23488): tensor(0.0527, device='cuda:0'),\n",
       " (0, 'M', 1, 95): tensor(3.7466, device='cuda:0'),\n",
       " (0, 'M', 15, 14036): tensor(1.0165, device='cuda:0'),\n",
       " (0, 'M', 15, 10329): tensor(0.6260, device='cuda:0'),\n",
       " (0, 'M', 14, 18079): tensor(0.2779, device='cuda:0'),\n",
       " (0, 'M', 4, 1169): tensor(0.1929, device='cuda:0'),\n",
       " (0, 'M', 9, 22594): tensor(0.1804, device='cuda:0'),\n",
       " (0, 'M', 12, 5919): tensor(0.1543, device='cuda:0'),\n",
       " (0, 'M', 15, 6317): tensor(0.1467, device='cuda:0'),\n",
       " (0, 'M', 3, 3705): tensor(0.1224, device='cuda:0'),\n",
       " (0, 'M', 3, 17659): tensor(0.1137, device='cuda:0'),\n",
       " (0, 'M', 15, 393): tensor(0.1040, device='cuda:0'),\n",
       " (0, 'M', 1, 17659): tensor(0.0988, device='cuda:0'),\n",
       " (0, 'M', 6, 13844): tensor(0.0938, device='cuda:0'),\n",
       " (0, 'M', 15, 14707): tensor(0.0892, device='cuda:0'),\n",
       " (0, 'M', 10, 22564): tensor(0.0888, device='cuda:0'),\n",
       " (0, 'M', 3, 13622): tensor(0.0845, device='cuda:0'),\n",
       " (0, 'M', 3, 5859): tensor(0.0842, device='cuda:0'),\n",
       " (0, 'M', 9, 17659): tensor(0.0808, device='cuda:0'),\n",
       " (0, 'M', 15, 13856): tensor(0.0807, device='cuda:0'),\n",
       " (0, 'M', 6, 5735): tensor(0.0734, device='cuda:0'),\n",
       " (0, 'M', 3, 21383): tensor(0.0522, device='cuda:0'),\n",
       " (1, 'A', 15, 17794): tensor(0.4109, device='cuda:0'),\n",
       " (1, 'A', 3, 18572): tensor(0.0677, device='cuda:0'),\n",
       " (1, 'A', 3, 19379): tensor(0.0658, device='cuda:0'),\n",
       " (1, 'A', 15, 15164): tensor(0.0572, device='cuda:0'),\n",
       " (1, 'A', 15, 2121): tensor(0.0557, device='cuda:0'),\n",
       " (1, 'M', 0, 18788): tensor(0.4128, device='cuda:0'),\n",
       " (1, 'M', 1, 3288): tensor(0.2943, device='cuda:0'),\n",
       " (1, 'M', 15, 3945): tensor(0.0950, device='cuda:0'),\n",
       " (1, 'M', 9, 1073): tensor(0.0942, device='cuda:0'),\n",
       " (1, 'M', 14, 23388): tensor(0.0836, device='cuda:0'),\n",
       " (1, 'M', 15, 19360): tensor(0.0775, device='cuda:0'),\n",
       " (1, 'M', 1, 17713): tensor(0.0763, device='cuda:0'),\n",
       " (1, 'M', 15, 15132): tensor(0.0515, device='cuda:0'),\n",
       " (2, 'A', 15, 13880): tensor(0.0725, device='cuda:0'),\n",
       " (2, 'A', 15, 22348): tensor(0.0515, device='cuda:0'),\n",
       " (2, 'M', 0, 17031): tensor(0.4024, device='cuda:0'),\n",
       " (2, 'M', 1, 12355): tensor(0.3413, device='cuda:0'),\n",
       " (2, 'M', 15, 6390): tensor(0.0680, device='cuda:0'),\n",
       " (2, 'M', 15, 7777): tensor(0.0651, device='cuda:0'),\n",
       " (3, 'M', 1, 22937): tensor(0.3242, device='cuda:0'),\n",
       " (3, 'M', 0, 6932): tensor(0.0739, device='cuda:0'),\n",
       " (3, 'M', 1, 14458): tensor(0.0674, device='cuda:0'),\n",
       " (3, 'M', 15, 5856): tensor(0.0509, device='cuda:0'),\n",
       " (4, 'A', 10, 16310): tensor(0.0511, device='cuda:0'),\n",
       " (4, 'M', 1, 3643): tensor(0.1726, device='cuda:0'),\n",
       " (4, 'M', 15, 4610): tensor(0.0625, device='cuda:0'),\n",
       " (5, 'M', 1, 6874): tensor(0.2250, device='cuda:0'),\n",
       " (6, 'A', 10, 18286): tensor(0.1313, device='cuda:0'),\n",
       " (6, 'A', 15, 3959): tensor(0.0653, device='cuda:0'),\n",
       " (6, 'M', 1, 21513): tensor(0.2678, device='cuda:0'),\n",
       " (6, 'M', 1, 7303): tensor(0.0919, device='cuda:0'),\n",
       " (6, 'M', 15, 20865): tensor(0.0667, device='cuda:0'),\n",
       " (6, 'M', 15, 14173): tensor(0.0655, device='cuda:0'),\n",
       " (7, 'A', 15, 23): tensor(0.2893, device='cuda:0'),\n",
       " (7, 'M', 1, 5063): tensor(0.1982, device='cuda:0'),\n",
       " (7, 'M', 15, 10702): tensor(0.0665, device='cuda:0'),\n",
       " (7, 'M', 1, 21775): tensor(0.0597, device='cuda:0'),\n",
       " (8, 'A', 15, 20056): tensor(0.1379, device='cuda:0'),\n",
       " (8, 'A', 15, 17578): tensor(0.0728, device='cuda:0'),\n",
       " (8, 'A', 15, 5725): tensor(0.0707, device='cuda:0'),\n",
       " (8, 'A', 15, 15578): tensor(0.0604, device='cuda:0'),\n",
       " (8, 'A', 15, 3183): tensor(0.0504, device='cuda:0'),\n",
       " (8, 'M', 1, 14853): tensor(0.1335, device='cuda:0'),\n",
       " (8, 'M', 15, 21284): tensor(0.0994, device='cuda:0'),\n",
       " (8, 'M', 15, 5155): tensor(0.0626, device='cuda:0'),\n",
       " (9, 'A', 15, 15384): tensor(0.3506, device='cuda:0'),\n",
       " (9, 'A', 15, 14571): tensor(0.3228, device='cuda:0'),\n",
       " (9, 'A', 15, 14631): tensor(0.3207, device='cuda:0'),\n",
       " (9, 'A', 15, 9767): tensor(0.2223, device='cuda:0'),\n",
       " (9, 'A', 15, 22905): tensor(0.1913, device='cuda:0'),\n",
       " (9, 'A', 15, 1853): tensor(0.0951, device='cuda:0'),\n",
       " (9, 'A', 15, 24519): tensor(0.0849, device='cuda:0'),\n",
       " (9, 'A', 15, 19865): tensor(0.0671, device='cuda:0'),\n",
       " (9, 'A', 15, 21367): tensor(0.0584, device='cuda:0'),\n",
       " (9, 'A', 15, 6698): tensor(0.0511, device='cuda:0'),\n",
       " (9, 'A', 15, 10935): tensor(0.0504, device='cuda:0'),\n",
       " (9, 'M', 15, 22685): tensor(0.0922, device='cuda:0'),\n",
       " (9, 'M', 15, 13117): tensor(0.0864, device='cuda:0'),\n",
       " (9, 'M', 15, 9354): tensor(0.0838, device='cuda:0'),\n",
       " (9, 'M', 15, 8562): tensor(0.0830, device='cuda:0'),\n",
       " (9, 'M', 15, 18380): tensor(0.0757, device='cuda:0'),\n",
       " (9, 'M', 15, 7804): tensor(0.0649, device='cuda:0'),\n",
       " (9, 'M', 15, 13854): tensor(0.0576, device='cuda:0'),\n",
       " (10, 'A', 15, 21737): tensor(0.0760, device='cuda:0'),\n",
       " (10, 'M', 15, 6261): tensor(0.0798, device='cuda:0'),\n",
       " (10, 'M', 15, 10298): tensor(0.0661, device='cuda:0'),\n",
       " (10, 'M', 15, 16664): tensor(0.0654, device='cuda:0'),\n",
       " (10, 'M', 15, 6328): tensor(0.0613, device='cuda:0'),\n",
       " (10, 'M', 15, 17866): tensor(0.0593, device='cuda:0'),\n",
       " (10, 'M', 15, 8727): tensor(0.0525, device='cuda:0'),\n",
       " (11, 'A', 15, 10431): tensor(0.1248, device='cuda:0'),\n",
       " (11, 'M', 15, 7047): tensor(0.1086, device='cuda:0'),\n",
       " (11, 'M', 15, 1724): tensor(0.0623, device='cuda:0'),\n",
       " (11, 'M', 15, 14619): tensor(0.0605, device='cuda:0')}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attributions_of_each_single_neuron = {}\n",
    "\n",
    "start = 0\n",
    "threshold = 0.05\n",
    "with torch.no_grad():\n",
    "    for i, block in enumerate(model.blocks):\n",
    "        feature_acts = block.attn_sae(cache[f'blocks.{start + i}.hook_attn_out'])[1][1]['feature_acts']\n",
    "        # print(block.attn_grad.size())\n",
    "        # print(block.attn_sae.decoder.size())\n",
    "        # print(feature_acts.size())\n",
    "        # print(block.attn_grad.norm(2, dim=-1))\n",
    "        attr = block.attn_grad[0, :, None, :] * block.attn_sae.decoder[None, :, :] * feature_acts[0, :, :, None]\n",
    "        attr = attr.sum(-1).flatten()\n",
    "        attr = attr.topk(30)\n",
    "        # print(attr.values)\n",
    "        \n",
    "        for idx, value in zip(attr.indices, attr.values):\n",
    "            if value > threshold:\n",
    "                token_idx, head_idx = divmod(idx.item(), block.attn_sae.cfg.d_sae)\n",
    "                attributions_of_each_single_neuron[(start + i, 'A', token_idx, head_idx)] = value\n",
    "\n",
    "        feature_acts = block.mlp_sae(cache[f'blocks.{start + i}.hook_resid_mid'], label=cache[f'blocks.{start + i}.hook_mlp_out'])[1][1]['feature_acts']\n",
    "        attr = block.mlp_grad[0, :, None, :] * block.mlp_sae.decoder[None, :, :] * feature_acts[0, :, :, None]\n",
    "        attr = attr.sum(-1).flatten()\n",
    "        attr = attr.topk(30)\n",
    "        \n",
    "        for idx, value in zip(attr.indices, attr.values):\n",
    "            if value > threshold:\n",
    "                token_idx, head_idx = divmod(idx.item(), block.mlp_sae.cfg.d_sae)\n",
    "                attributions_of_each_single_neuron[(start + i, 'M', token_idx, head_idx)] = value\n",
    "\n",
    "attributions_of_each_single_neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
