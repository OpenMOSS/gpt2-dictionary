{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2 into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "from typing import Any, Callable\n",
    "\n",
    "sys.path.insert(0, os.getcwd())\n",
    "\n",
    "from HookedTransformer import HookedTransformer\n",
    "# from transformer_lens import HookedTransformer\n",
    "\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "import networkx as nx\n",
    "import random\n",
    "import math\n",
    "import pickle\n",
    "import dataclasses\n",
    "import numpy as np\n",
    "\n",
    "from einops import repeat\n",
    "\n",
    "import plotly.express as px\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from core.config import SAEConfig\n",
    "from core.sae import SparseAutoEncoder\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "hf_model = AutoModelForCausalLM.from_pretrained('gpt2')\n",
    "model = HookedTransformer.from_pretrained('gpt2', device=device, hf_model=hf_model)\n",
    "\n",
    "def check_all_close():\n",
    "\timport transformer_lens\n",
    "\torigin_tl_model = transformer_lens.HookedTransformer.from_pretrained('gpt2', device=device, hf_model=hf_model)\n",
    "\tlogits = model(model.to_tokens('Hello, World.'))\n",
    "\torigin_logits = origin_tl_model(origin_tl_model.to_tokens('Hello, World.'))\n",
    "\tassert torch.allclose(logits, origin_logits, atol=1e-4), f\"Logits are not close: {logits} != {origin_logits}\"\n",
    "\n",
    "# input = model.to_tokens(\" OpenMoss! OpenMoss! OpenMoss!\", prepend_bos=False)\n",
    "# input = model.to_tokens(\"Outside [Inside] Outside\", prepend_bos=False)\n",
    "# input = model.to_tokens(\"0 0 [1 1 1 [2] 3] 4\", prepend_bos=False)\n",
    "# input = model.to_tokens(\"Video in WebM support: Your browser doesn't support HTML5 video in WebM.\", prepend_bos=False)\n",
    "# input = model.to_tokens(\"Form-fitting TrekDry helps keep hands cool and comfortable. Form-fitting TrekDry material is lightweight and breathable.\", prepend_bos=False)\n",
    "# input = model.to_tokens(\" it was its command line interface. You get so much leverage by being able to scaffold a [Inner Inner] A B A\", prepend_bos=False)\n",
    "# input = model.to_tokens(\"[[[ OpenMoss ]]] OpenMoss Open Moss ]\", prepend_bos=False)\n",
    "# input = model.to_tokens(\"Fruits:\\n\\napple red\\n\\nbanana yellow\\n\\ngrape purple\", prepend_bos=False)\n",
    "# input = model.to_tokens(\"Fruits:\\n\\nbanana yellow\\n\\napple red\\n\\ngrape purple\", prepend_bos=False)\n",
    "# input = model.to_tokens(\"Youâ€™re used to endlessly circular debates where Republican shills and Democratic shills\", prepend_bos=False)\n",
    "# input = model.to_tokens(\"Afterwards, Alice and Tom went to the shop. Tom gave a bunch of flowers to\", prepend_bos=False)\n",
    "# input = model.to_tokens(\"Afterwards, Tom and Alice went to the shop. Tom gave a bunch of flowers to\", prepend_bos=False)\n",
    "# input = model.to_tokens(\"When Mary and John went to the store, Mary gave a bottle of milk to\", prepend_bos=False)\n",
    "# input = model.to_tokens(\"When Mary and John went to the store, John gave a bottle of milk to\", prepend_bos=False)\n",
    "# input = model.to_tokens(\"When John and Mary went to the store, John gave a bottle of milk to\", prepend_bos=False)\n",
    "# input = model.to_tokens(\"20 Parts Rosemary, 8 Parts Grapefruit\", prepend_bos=False)\n",
    "\n",
    "# answer = model.to_tokens(\" Mary\", prepend_bos=False)\n",
    "# assert answer.size(0) == 1\n",
    "# logits, cache = model.run_with_cache(input)\n",
    "# logits = logits[0, -1, answer.item()]\n",
    "# print(logits)\n",
    "# logits.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 0 Nodes: 9667 Contribution: 14.758073806762695\n",
      "Threshold: 1e-05 Nodes: 8770 Contribution: 14.576600074768066\n",
      "Threshold: 3e-05 Nodes: 7929 Contribution: 14.203865051269531\n",
      "Threshold: 0.0001 Nodes: 6411 Contribution: 13.222742080688477\n",
      "Threshold: 0.0003 Nodes: 4487 Contribution: 11.559713363647461\n",
      "Threshold: 0.001 Nodes: 2252 Contribution: 8.495205879211426\n",
      "Threshold: 0.003 Nodes: 899 Contribution: 6.189325332641602\n",
      "Threshold: 0.01 Nodes: 380 Contribution: 3.173037052154541\n",
      "Threshold: 0.03 Nodes: 222 Contribution: 0.955620288848877\n",
      "Threshold: 0.1 Nodes: 157 Contribution: 0.0\n"
     ]
    }
   ],
   "source": [
    "import einops\n",
    "\n",
    "for threshold in [0, 1e-5, 3e-5, 1e-4, 3e-4, 1e-3, 3e-3, 1e-2, 3e-2, 1e-1]:\n",
    "    model.cfg.detach_pattern = True\n",
    "    model.cfg.add_sae_error = True\n",
    "    model.cfg.prune_on_backward = True\n",
    "    model.cfg.prune_on_backward_threshold = threshold\n",
    "\n",
    "    input = model.to_tokens(\"When Mary and John went to the store, John gave a bottle of milk to\", prepend_bos=False)\n",
    "    # input = model.to_tokens(\"When John and Mary went to the store, John gave a bottle of milk to\", prepend_bos=False)\n",
    "    # input = model.to_tokens(\"20 Parts Rosemary, 8 Parts Grapefruit\", prepend_bos=False)\n",
    "\n",
    "    answer = model.to_tokens(\" Mary\", prepend_bos=False)\n",
    "    wrong_answer = model.to_tokens(\" John\", prepend_bos=False)\n",
    "    assert answer.size(0) == 1\n",
    "    cache = model.add_caching_hooks(incl_bwd=True)\n",
    "    logits = model(input)\n",
    "    true_logits = logits[0, -1, answer.item()]\n",
    "    wrong_logits = logits[0, -1, wrong_answer.item()]\n",
    "    model.zero_grad()\n",
    "    # true_logits.backward()\n",
    "    target = true_logits - wrong_logits\n",
    "    target.backward()\n",
    "\n",
    "    nodes = 0\n",
    "    contribution = torch.zeros_like(true_logits)\n",
    "    error_contribution = torch.zeros_like(true_logits)\n",
    "\n",
    "    embed_contribution = einops.einsum(cache[\"hook_embed\"], cache[\"hook_embed_grad\"], \"b l d, b l d -> b l\")\n",
    "    nodes += (embed_contribution[0] > model.cfg.prune_on_backward_threshold).nonzero(as_tuple=True)[0].shape[0]\n",
    "    contribution += embed_contribution[embed_contribution > model.cfg.prune_on_backward_threshold].sum()\n",
    "\n",
    "    pos_embed_contribution = einops.einsum(cache[\"hook_pos_embed\"], cache[\"hook_pos_embed_grad\"], \"b l d, b l d -> b l\")\n",
    "    nodes += (pos_embed_contribution[0] > model.cfg.prune_on_backward_threshold).nonzero(as_tuple=True)[0].shape[0]\n",
    "    contribution += pos_embed_contribution[pos_embed_contribution > model.cfg.prune_on_backward_threshold].sum()\n",
    "\n",
    "    for i in range(12):\n",
    "        attn_contribution = einops.einsum(cache[f\"blocks.{i}.hook_attn_feature_acts\"], cache[f\"blocks.{i}.hook_attn_feature_acts_grad\"], \"b l f, b l f -> b l f\")\n",
    "        nodes += (attn_contribution[0] > model.cfg.prune_on_backward_threshold).reshape(-1).nonzero(as_tuple=True)[0].shape[0]\n",
    "\n",
    "        attn_sae_error_contribution = einops.einsum(cache[f\"blocks.{i}.hook_attn_sae_error\"], cache[f\"blocks.{i}.hook_attn_sae_error_grad\"], \"b l d, b l d -> b l\")\n",
    "        error_contribution += attn_sae_error_contribution.sum()\n",
    "\n",
    "        b_V_contribution = einops.einsum(model.blocks[i].attn.b_V, model.blocks[i].attn.b_V.grad, \"h d, h d -> h\")\n",
    "        nodes += (b_V_contribution > model.cfg.prune_on_backward_threshold).nonzero(as_tuple=True)[0].shape[0]\n",
    "        contribution += b_V_contribution[b_V_contribution > model.cfg.prune_on_backward_threshold].sum()\n",
    "\n",
    "        attn_b_E_contribution = einops.einsum(model.blocks[i].attn_sae.encoder_bias, model.blocks[i].attn_sae.encoder_bias.grad, \"f, f -> \")\n",
    "        nodes += (attn_b_E_contribution > model.cfg.prune_on_backward_threshold).nonzero(as_tuple=True)[0].shape[0]\n",
    "        contribution += attn_b_E_contribution[attn_b_E_contribution > model.cfg.prune_on_backward_threshold].sum()\n",
    "\n",
    "        mlp_contribution = einops.einsum(cache[f\"blocks.{i}.hook_mlp_feature_acts\"], cache[f\"blocks.{i}.hook_mlp_feature_acts_grad\"], \"b l f, b l f -> b l\")\n",
    "        nodes += (mlp_contribution[0] > model.cfg.prune_on_backward_threshold).reshape(-1).nonzero(as_tuple=True)[0].shape[0]\n",
    "\n",
    "        mlp_sae_error_contribution = einops.einsum(cache[f\"blocks.{i}.hook_mlp_sae_error\"], cache[f\"blocks.{i}.hook_mlp_sae_error_grad\"], \"b l d, b l d -> b l\")\n",
    "        error_contribution += mlp_sae_error_contribution.sum()\n",
    "\n",
    "        mlp_b_E_contribution = einops.einsum(model.blocks[i].mlp_sae.encoder_bias, model.blocks[i].mlp_sae.encoder_bias.grad, \"f, f -> \")\n",
    "        nodes += (mlp_b_E_contribution > model.cfg.prune_on_backward_threshold).nonzero(as_tuple=True)[0].shape[0]\n",
    "        contribution += mlp_b_E_contribution[mlp_b_E_contribution > model.cfg.prune_on_backward_threshold].sum()\n",
    "\n",
    "    print(\"Threshold:\", threshold, \"Nodes:\", nodes, \"Contribution:\", contribution.item())\n",
    "\n",
    "print(\"Target:\", target.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "\n",
    "\n",
    "print((cache[\"blocks.5.hook_attn_feature_acts_grad\"] * cache[\"blocks.5.hook_attn_feature_acts\"] > 0)[0][-1].nonzero(as_tuple=True)[0].shape)\n",
    "print(einops.einsum(cache[f\"blocks.11.hook_mlp_sae_error\"], cache[f\"blocks.11.hook_mlp_sae_error_grad\"], \"b l d, b l d -> b l\"))\n",
    "print(cache[f\"blocks.11.hook_mlp_sae_error\"][0][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506\n",
      "tensor(7.7001, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2317419061.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[102], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    feature_acts = model.L0RPr_sae(cache['blocks.0.hook_resid_pre'])[][1]['feature_acts']\u001b[0m\n\u001b[0m                                                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "feature_acts = model.L0RPr_sae(cache['blocks.0.hook_resid_pre'])[][1]['feature_acts']\n",
    "attr = model.L0RPr_grad[0, :, None, :] * model.L0RPr_sae.decoder[None, :, :] * feature_acts[0, :, :, None]\n",
    "attr = attr.sum(-1).flatten()\n",
    "attr = attr.topk(30)\n",
    "for idx, value in zip(attr.indices, attr.values):\n",
    "\tif value > threshold:\n",
    "\t\ttoken_idx, head_idx = divmod(idx.item(), block.attn_sae.cfg.d_sae)\n",
    "\t\tprint(token_idx, head_idx, value)\n",
    "# (model.L0RPr_grad * model.L0RPr_sae(cache['blocks.0.hook_resid_pre'])[1][1]['x_hat']).sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 'A', 0, 20459): tensor(2.5407, device='cuda:0'),\n",
       " (0, 'A', 3, 11601): tensor(1.8369, device='cuda:0'),\n",
       " (0, 'A', 3, 8101): tensor(0.4803, device='cuda:0'),\n",
       " (0, 'A', 1, 14671): tensor(0.4639, device='cuda:0'),\n",
       " (0, 'A', 8, 23630): tensor(0.4037, device='cuda:0'),\n",
       " (0, 'A', 3, 19931): tensor(0.3042, device='cuda:0'),\n",
       " (0, 'A', 1, 18977): tensor(0.2797, device='cuda:0'),\n",
       " (0, 'A', 8, 19931): tensor(0.2788, device='cuda:0'),\n",
       " (0, 'A', 8, 13741): tensor(0.2101, device='cuda:0'),\n",
       " (0, 'A', 8, 14768): tensor(0.1655, device='cuda:0'),\n",
       " (0, 'A', 1, 4342): tensor(0.1181, device='cuda:0'),\n",
       " (0, 'A', 1, 9204): tensor(0.1048, device='cuda:0'),\n",
       " (0, 'A', 3, 12731): tensor(0.1001, device='cuda:0'),\n",
       " (0, 'A', 1, 6617): tensor(0.0777, device='cuda:0'),\n",
       " (0, 'A', 3, 10869): tensor(0.0759, device='cuda:0'),\n",
       " (0, 'A', 8, 8101): tensor(0.0746, device='cuda:0'),\n",
       " (0, 'A', 0, 14131): tensor(0.0733, device='cuda:0'),\n",
       " (0, 'A', 3, 24377): tensor(0.0725, device='cuda:0'),\n",
       " (0, 'A', 1, 11425): tensor(0.0713, device='cuda:0'),\n",
       " (0, 'A', 8, 11601): tensor(0.0686, device='cuda:0'),\n",
       " (0, 'A', 1, 10869): tensor(0.0670, device='cuda:0'),\n",
       " (0, 'A', 3, 20129): tensor(0.0574, device='cuda:0'),\n",
       " (0, 'A', 1, 23488): tensor(0.0527, device='cuda:0'),\n",
       " (0, 'M', 1, 95): tensor(3.7466, device='cuda:0'),\n",
       " (0, 'M', 15, 14036): tensor(1.0165, device='cuda:0'),\n",
       " (0, 'M', 15, 10329): tensor(0.6260, device='cuda:0'),\n",
       " (0, 'M', 14, 18079): tensor(0.2779, device='cuda:0'),\n",
       " (0, 'M', 4, 1169): tensor(0.1929, device='cuda:0'),\n",
       " (0, 'M', 9, 22594): tensor(0.1804, device='cuda:0'),\n",
       " (0, 'M', 12, 5919): tensor(0.1543, device='cuda:0'),\n",
       " (0, 'M', 15, 6317): tensor(0.1467, device='cuda:0'),\n",
       " (0, 'M', 3, 3705): tensor(0.1224, device='cuda:0'),\n",
       " (0, 'M', 3, 17659): tensor(0.1137, device='cuda:0'),\n",
       " (0, 'M', 15, 393): tensor(0.1040, device='cuda:0'),\n",
       " (0, 'M', 1, 17659): tensor(0.0988, device='cuda:0'),\n",
       " (0, 'M', 6, 13844): tensor(0.0938, device='cuda:0'),\n",
       " (0, 'M', 15, 14707): tensor(0.0892, device='cuda:0'),\n",
       " (0, 'M', 10, 22564): tensor(0.0888, device='cuda:0'),\n",
       " (0, 'M', 3, 13622): tensor(0.0845, device='cuda:0'),\n",
       " (0, 'M', 3, 5859): tensor(0.0842, device='cuda:0'),\n",
       " (0, 'M', 9, 17659): tensor(0.0808, device='cuda:0'),\n",
       " (0, 'M', 15, 13856): tensor(0.0807, device='cuda:0'),\n",
       " (0, 'M', 6, 5735): tensor(0.0734, device='cuda:0'),\n",
       " (0, 'M', 3, 21383): tensor(0.0522, device='cuda:0'),\n",
       " (1, 'A', 15, 17794): tensor(0.4109, device='cuda:0'),\n",
       " (1, 'A', 3, 18572): tensor(0.0677, device='cuda:0'),\n",
       " (1, 'A', 3, 19379): tensor(0.0658, device='cuda:0'),\n",
       " (1, 'A', 15, 15164): tensor(0.0572, device='cuda:0'),\n",
       " (1, 'A', 15, 2121): tensor(0.0557, device='cuda:0'),\n",
       " (1, 'M', 0, 18788): tensor(0.4128, device='cuda:0'),\n",
       " (1, 'M', 1, 3288): tensor(0.2943, device='cuda:0'),\n",
       " (1, 'M', 15, 3945): tensor(0.0950, device='cuda:0'),\n",
       " (1, 'M', 9, 1073): tensor(0.0942, device='cuda:0'),\n",
       " (1, 'M', 14, 23388): tensor(0.0836, device='cuda:0'),\n",
       " (1, 'M', 15, 19360): tensor(0.0775, device='cuda:0'),\n",
       " (1, 'M', 1, 17713): tensor(0.0763, device='cuda:0'),\n",
       " (1, 'M', 15, 15132): tensor(0.0515, device='cuda:0'),\n",
       " (2, 'A', 15, 13880): tensor(0.0725, device='cuda:0'),\n",
       " (2, 'A', 15, 22348): tensor(0.0515, device='cuda:0'),\n",
       " (2, 'M', 0, 17031): tensor(0.4024, device='cuda:0'),\n",
       " (2, 'M', 1, 12355): tensor(0.3413, device='cuda:0'),\n",
       " (2, 'M', 15, 6390): tensor(0.0680, device='cuda:0'),\n",
       " (2, 'M', 15, 7777): tensor(0.0651, device='cuda:0'),\n",
       " (3, 'M', 1, 22937): tensor(0.3242, device='cuda:0'),\n",
       " (3, 'M', 0, 6932): tensor(0.0739, device='cuda:0'),\n",
       " (3, 'M', 1, 14458): tensor(0.0674, device='cuda:0'),\n",
       " (3, 'M', 15, 5856): tensor(0.0509, device='cuda:0'),\n",
       " (4, 'A', 10, 16310): tensor(0.0511, device='cuda:0'),\n",
       " (4, 'M', 1, 3643): tensor(0.1726, device='cuda:0'),\n",
       " (4, 'M', 15, 4610): tensor(0.0625, device='cuda:0'),\n",
       " (5, 'M', 1, 6874): tensor(0.2250, device='cuda:0'),\n",
       " (6, 'A', 10, 18286): tensor(0.1313, device='cuda:0'),\n",
       " (6, 'A', 15, 3959): tensor(0.0653, device='cuda:0'),\n",
       " (6, 'M', 1, 21513): tensor(0.2678, device='cuda:0'),\n",
       " (6, 'M', 1, 7303): tensor(0.0919, device='cuda:0'),\n",
       " (6, 'M', 15, 20865): tensor(0.0667, device='cuda:0'),\n",
       " (6, 'M', 15, 14173): tensor(0.0655, device='cuda:0'),\n",
       " (7, 'A', 15, 23): tensor(0.2893, device='cuda:0'),\n",
       " (7, 'M', 1, 5063): tensor(0.1982, device='cuda:0'),\n",
       " (7, 'M', 15, 10702): tensor(0.0665, device='cuda:0'),\n",
       " (7, 'M', 1, 21775): tensor(0.0597, device='cuda:0'),\n",
       " (8, 'A', 15, 20056): tensor(0.1379, device='cuda:0'),\n",
       " (8, 'A', 15, 17578): tensor(0.0728, device='cuda:0'),\n",
       " (8, 'A', 15, 5725): tensor(0.0707, device='cuda:0'),\n",
       " (8, 'A', 15, 15578): tensor(0.0604, device='cuda:0'),\n",
       " (8, 'A', 15, 3183): tensor(0.0504, device='cuda:0'),\n",
       " (8, 'M', 1, 14853): tensor(0.1335, device='cuda:0'),\n",
       " (8, 'M', 15, 21284): tensor(0.0994, device='cuda:0'),\n",
       " (8, 'M', 15, 5155): tensor(0.0626, device='cuda:0'),\n",
       " (9, 'A', 15, 15384): tensor(0.3506, device='cuda:0'),\n",
       " (9, 'A', 15, 14571): tensor(0.3228, device='cuda:0'),\n",
       " (9, 'A', 15, 14631): tensor(0.3207, device='cuda:0'),\n",
       " (9, 'A', 15, 9767): tensor(0.2223, device='cuda:0'),\n",
       " (9, 'A', 15, 22905): tensor(0.1913, device='cuda:0'),\n",
       " (9, 'A', 15, 1853): tensor(0.0951, device='cuda:0'),\n",
       " (9, 'A', 15, 24519): tensor(0.0849, device='cuda:0'),\n",
       " (9, 'A', 15, 19865): tensor(0.0671, device='cuda:0'),\n",
       " (9, 'A', 15, 21367): tensor(0.0584, device='cuda:0'),\n",
       " (9, 'A', 15, 6698): tensor(0.0511, device='cuda:0'),\n",
       " (9, 'A', 15, 10935): tensor(0.0504, device='cuda:0'),\n",
       " (9, 'M', 15, 22685): tensor(0.0922, device='cuda:0'),\n",
       " (9, 'M', 15, 13117): tensor(0.0864, device='cuda:0'),\n",
       " (9, 'M', 15, 9354): tensor(0.0838, device='cuda:0'),\n",
       " (9, 'M', 15, 8562): tensor(0.0830, device='cuda:0'),\n",
       " (9, 'M', 15, 18380): tensor(0.0757, device='cuda:0'),\n",
       " (9, 'M', 15, 7804): tensor(0.0649, device='cuda:0'),\n",
       " (9, 'M', 15, 13854): tensor(0.0576, device='cuda:0'),\n",
       " (10, 'A', 15, 21737): tensor(0.0760, device='cuda:0'),\n",
       " (10, 'M', 15, 6261): tensor(0.0798, device='cuda:0'),\n",
       " (10, 'M', 15, 10298): tensor(0.0661, device='cuda:0'),\n",
       " (10, 'M', 15, 16664): tensor(0.0654, device='cuda:0'),\n",
       " (10, 'M', 15, 6328): tensor(0.0613, device='cuda:0'),\n",
       " (10, 'M', 15, 17866): tensor(0.0593, device='cuda:0'),\n",
       " (10, 'M', 15, 8727): tensor(0.0525, device='cuda:0'),\n",
       " (11, 'A', 15, 10431): tensor(0.1248, device='cuda:0'),\n",
       " (11, 'M', 15, 7047): tensor(0.1086, device='cuda:0'),\n",
       " (11, 'M', 15, 1724): tensor(0.0623, device='cuda:0'),\n",
       " (11, 'M', 15, 14619): tensor(0.0605, device='cuda:0')}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attributions_of_each_single_neuron = {}\n",
    "\n",
    "start = 0\n",
    "threshold = 0.05\n",
    "with torch.no_grad():\n",
    "    for i, block in enumerate(model.blocks):\n",
    "        feature_acts = block.attn_sae(cache[f'blocks.{start + i}.hook_attn_out'])[1][1]['feature_acts']\n",
    "        # print(block.attn_grad.size())\n",
    "        # print(block.attn_sae.decoder.size())\n",
    "        # print(feature_acts.size())\n",
    "        # print(block.attn_grad.norm(2, dim=-1))\n",
    "        attr = block.attn_grad[0, :, None, :] * block.attn_sae.decoder[None, :, :] * feature_acts[0, :, :, None]\n",
    "        attr = attr.sum(-1).flatten()\n",
    "        attr = attr.topk(30)\n",
    "        # print(attr.values)\n",
    "        \n",
    "        for idx, value in zip(attr.indices, attr.values):\n",
    "            if value > threshold:\n",
    "                token_idx, head_idx = divmod(idx.item(), block.attn_sae.cfg.d_sae)\n",
    "                attributions_of_each_single_neuron[(start + i, 'A', token_idx, head_idx)] = value\n",
    "\n",
    "        feature_acts = block.mlp_sae(cache[f'blocks.{start + i}.hook_resid_mid'], label=cache[f'blocks.{start + i}.hook_mlp_out'])[1][1]['feature_acts']\n",
    "        attr = block.mlp_grad[0, :, None, :] * block.mlp_sae.decoder[None, :, :] * feature_acts[0, :, :, None]\n",
    "        attr = attr.sum(-1).flatten()\n",
    "        attr = attr.topk(30)\n",
    "        \n",
    "        for idx, value in zip(attr.indices, attr.values):\n",
    "            if value > threshold:\n",
    "                token_idx, head_idx = divmod(idx.item(), block.mlp_sae.cfg.d_sae)\n",
    "                attributions_of_each_single_neuron[(start + i, 'M', token_idx, head_idx)] = value\n",
    "\n",
    "attributions_of_each_single_neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
